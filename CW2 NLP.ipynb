{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22d3ecb0-3e0e-4cac-94f7-77a2ed1e1818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chinnu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30244a2a-a3b7-47c4-a6e8-57636452c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2013dev-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2013test-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2013train-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2014sarcasm-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2014test-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2015test-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2015train-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2016dev-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2016devtest-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2016test-A.txt\",\n",
    "    \"/Users/chinnu/Desktop/project/Gopi/2017_English_final/2017_English_final/GOLD/Subtask_A/twitter-2016train-A.txt\",\n",
    "]\n",
    "\n",
    "# Load and combine data safely\n",
    "data = []\n",
    "for file in files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 3 and parts[1].lower() in [\"positive\", \"neutral\", \"negative\"]:\n",
    "                data.append(parts)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"id\", \"label\", \"tweet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7988f998-836e-45fb-b0a0-39808404c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your dataset\n",
    "\n",
    "df = df.dropna()\n",
    "df = df[df['label'].isin(['positive', 'neutral', 'negative'])]  # sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4775bda5-eb7b-49b7-a26d-90d052d7bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label_enc\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "# Split data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"tweet\"].tolist(), df[\"label_enc\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6820e3a-a7fd-40a9-909d-0bdf294fdf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n",
    "            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n",
    "            \"labels\": torch.tensor(self.labels[idx]),\n",
    "        }\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56145b0e-8b5d-4e5b-a78a-29328d73a9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f26c1311c394b8db8226712c4201d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f1edf12-2fd9-44ba-a0c0-18fd18390ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    print(\"\\n\" + classification_report(labels, preds, target_names=label_encoder.classes_))\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",             \n",
    "    save_strategy=\"epoch\",             \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2ac0cf0-9a2d-48a7-9686-0ec0b28d1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b05a7a15-a221-484e-8551-76fde801fb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2517' max='2517' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2517/2517 44:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.585284</td>\n",
       "      <td>0.739593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.66      0.67      1549\n",
      "     neutral       0.74      0.72      0.73      4531\n",
      "    positive       0.77      0.79      0.78      3985\n",
      "\n",
      "    accuracy                           0.74     10065\n",
      "   macro avg       0.73      0.72      0.72     10065\n",
      "weighted avg       0.74      0.74      0.74     10065\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 02:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.66      0.67      1549\n",
      "     neutral       0.74      0.72      0.73      4531\n",
      "    positive       0.77      0.79      0.78      3985\n",
      "\n",
      "    accuracy                           0.74     10065\n",
      "   macro avg       0.73      0.72      0.72     10065\n",
      "weighted avg       0.74      0.74      0.74     10065\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5852842330932617,\n",
       " 'eval_accuracy': 0.7395926477893691,\n",
       " 'eval_runtime': 179.0494,\n",
       " 'eval_samples_per_second': 56.214,\n",
       " 'eval_steps_per_second': 3.519,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86f25e-bcb6-462d-b138-07c746b180b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c6196-e63b-47d9-b2ae-90f8c018e688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33983eb4-0cbd-4f17-9d0d-2b4365440ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71b42c38-874e-410e-9cc2-e0c2af006caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label_enc\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "#  Basic whitespace tokenization\n",
    "df[\"tokens\"] = df[\"tweet\"].apply(lambda x: x.lower().split())\n",
    "\n",
    "all_tokens = [token for tokens in df[\"tokens\"] for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "vocab = {word: i + 2 for i, (word, freq) in enumerate(vocab_counter.items()) if freq >= 2}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "vocab_size = max(vocab.values()) + 1  # Final vocab size for Embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4bfe6b4-d5c5-4e27-a82d-5615da518b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Encode function\n",
    "def encode(tokens, vocab, max_len=32):\n",
    "    encoded = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "    return encoded[:max_len] + [vocab[\"<PAD>\"]] * (max_len - len(encoded))\n",
    "\n",
    "df[\"input_ids\"] = df[\"tokens\"].apply(lambda x: encode(x, vocab))\n",
    "\n",
    "#  Dataset\n",
    "class TweetFastTextDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42077d75-5f47-4619-85ff-a0314cebdae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"input_ids\"].tolist(), df[\"label_enc\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TweetFastTextDataset(X_train, y_train)\n",
    "test_dataset = TweetFastTextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea632491-7bc7-431b-b968-54437d805cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… FastText Model\n",
    "class FastTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(FastTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)           # [batch, seq_len, embed_dim]\n",
    "        pooled = embeds.mean(dim=1)          # [batch, embed_dim]\n",
    "        return self.fc(pooled)               # [bat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e2c34fa0-b9ef-4136-b3f3-32072e90b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setup for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FastTextClassifier(vocab_size=vocab_size, embed_dim=100, num_classes=3).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9eaa87fc-232e-4000-87a5-87857ba8dca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 1222.4379\n",
      "Epoch 2 - Loss: 1028.7580\n",
      "Epoch 3 - Loss: 886.8214\n",
      "Epoch 4 - Loss: 779.8127\n",
      "Epoch 5 - Loss: 692.7348\n"
     ]
    }
   ],
   "source": [
    "#  Training Loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0922f4fb-744f-4abd-8f32-1a4bf6416e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.40      0.47      1549\n",
      "     neutral       0.63      0.70      0.67      4531\n",
      "    positive       0.68      0.68      0.68      3985\n",
      "\n",
      "    accuracy                           0.65     10065\n",
      "   macro avg       0.63      0.59      0.60     10065\n",
      "weighted avg       0.64      0.65      0.64     10065\n",
      "\n",
      "Accuracy: 0.6455042225534029\n"
     ]
    }
   ],
   "source": [
    "#  Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#  Final Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aba6f4-7585-440d-9ab5-384d5ff6204f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
